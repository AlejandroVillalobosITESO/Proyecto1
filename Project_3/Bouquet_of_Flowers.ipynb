{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project No. 3 - Bouquet of Flowers\n",
    "### Authors:\n",
    "M. Alejandro Villalobos C.\n",
    "Óscar Ruiz Ramirez\n",
    "Sofía Vargas Aceves\n",
    "### Fecha:\n",
    "24 de Marzo, 2022\n",
    "### Description:\n",
    "Tercer proyecto Machine Learning. Bouquet of Flowers Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA READ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9',\n",
       "       ...\n",
       "       'n2044', 'n2045', 'n2046', 'n2047', 'category', 'image name', 'image',\n",
       "       'size', 'width', 'height'],\n",
       "      dtype='object', length=2054)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Dataset = pd.read_csv(\"./Embedded_images.csv\")\n",
    "\n",
    "\n",
    "Dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset.drop(['image','image name', 'size', 'width','height'],axis=1)\n",
    "Dataset['category'] = Dataset['category'].replace(['Camelia', 'Dahlia', 'Hydrangea', 'Lilies', 'Orchids', 'Peony', 'Ranunculus', 'Roses', 'Sunflowers', 'Tulips'], [0,1,2,3,4,5,6,7,8,9])\n",
    "DataFrameFlowers = pd.DataFrame(Dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DataFrameFlowers.drop(['category'], axis = 1)\n",
    "Y = DataFrameFlowers['category']\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementado con SAGA por el tamaño de muestra, referenciando (6)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=50.0 / y_train.shape[0], penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
    "LR_better = LogisticRegression(C=50.0 / y_train.shape[0], penalty=\"l2\", solver=\"lbfgs\", tol=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training: Adjust Model with Historic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Code\\anaconda3\\envs\\DeepEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.24875621890547264)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(X_train, y_train)\n",
    "LR_better.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prediction for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictLR = LR.predict(X_test)\n",
    "y_predictLR_better = LR_better.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2 Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui haremos las variables especialmente para el modelo CNN, ya que se necesita estar en 3 dimensiones\n",
    "#Para training 201 x 2048 x 1, para test 51 x 2048 x 1\n",
    "#Código complementado con (5)\n",
    "X_trainCNN = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_testCNN = X_test.values.reshape(X_test.shape[0], X_test.shape[1] ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "\n",
    "#Para coincidir tamaños, se convierten las etiquetas en \"one hot vectors\"\n",
    "#Código complementado con (5)\n",
    "y_trainCNN = keras.utils.to_categorical(np.asarray(y_train.factorize()[0]))\n",
    "y_testCNN = keras.utils.to_categorical(np.asarray(y_test.factorize()[0]))\n",
    "\n",
    "CNN = Sequential()\n",
    "CNN.add(Conv1D(128, 3, activation='relu', input_shape=(2048,1)))\n",
    "CNN.add(MaxPooling1D((2)))\n",
    "CNN.add(Conv1D(256, 3, activation='relu'))\n",
    "CNN.add(MaxPooling1D((2)))\n",
    "CNN.add(Conv1D(256, 3, activation='relu'))\n",
    "CNN.add(MaxPooling1D((2)))\n",
    "\n",
    "CNN.add(Flatten())\n",
    "CNN.add(Dense(256, activation='relu'))\n",
    "\n",
    "#Capa Dense tamaño 10 por las 10 categorizaciones finales\n",
    "#Complementado con activación softmax según (7)\n",
    "CNN.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Complementado con loss según (7)\n",
    "CNN.compile(optimizer='adam', loss = keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = X_trainCNN.shape[1], X_trainCNN.shape[2], y_trainCNN.shape[1]\n",
    "CNN_better = Sequential()\n",
    "CNN_better.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "CNN_better.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "CNN_better.add(Dropout(0.7))\n",
    "CNN_better.add(MaxPooling1D(pool_size=2))\n",
    "CNN_better.add(Flatten())\n",
    "CNN_better.add(Dense(256, activation='relu'))\n",
    "\n",
    "#Capa Dense tamaño 10 por las 10 categorizaciones finales\n",
    "#Complementado con activación softmax según (7)\n",
    "CNN_better.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "#Complementado con loss según (7)\n",
    "CNN_better.compile(optimizer='adam', loss = keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 10)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainCNN.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training: Adjust Model with Historic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 5s 746ms/step - loss: 2.5153 - accuracy: 0.1493 - val_loss: 2.2982 - val_accuracy: 0.1569\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 4s 600ms/step - loss: 2.1568 - accuracy: 0.3433 - val_loss: 2.1997 - val_accuracy: 0.3725\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 5s 667ms/step - loss: 1.6505 - accuracy: 0.4975 - val_loss: 2.1409 - val_accuracy: 0.2941\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 5s 726ms/step - loss: 0.9918 - accuracy: 0.6716 - val_loss: 2.8746 - val_accuracy: 0.2745\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 6s 837ms/step - loss: 0.5380 - accuracy: 0.8607 - val_loss: 3.9956 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 5s 729ms/step - loss: 0.3674 - accuracy: 0.8657 - val_loss: 3.6519 - val_accuracy: 0.3725\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 4s 612ms/step - loss: 0.1723 - accuracy: 0.9602 - val_loss: 4.4047 - val_accuracy: 0.3529\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 4s 628ms/step - loss: 0.1167 - accuracy: 0.9751 - val_loss: 5.2800 - val_accuracy: 0.3529\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 4s 628ms/step - loss: 0.0721 - accuracy: 0.9851 - val_loss: 5.9026 - val_accuracy: 0.3333\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 4s 627ms/step - loss: 0.0758 - accuracy: 0.9801 - val_loss: 5.4562 - val_accuracy: 0.3529\n"
     ]
    }
   ],
   "source": [
    "CNN_H = CNN.fit(X_trainCNN, y_trainCNN, validation_data=(X_testCNN, y_testCNN), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 2s 343ms/step - loss: 2.7477 - accuracy: 0.2388 - val_loss: 2.5939 - val_accuracy: 0.1569\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.8660 - accuracy: 0.5323 - val_loss: 2.1788 - val_accuracy: 0.1961\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 3s 362ms/step - loss: 1.1228 - accuracy: 0.7363 - val_loss: 2.4393 - val_accuracy: 0.2353\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.5527 - accuracy: 0.8408 - val_loss: 2.6968 - val_accuracy: 0.3922\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.3094 - accuracy: 0.8806 - val_loss: 3.4925 - val_accuracy: 0.3137\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 2s 287ms/step - loss: 0.1940 - accuracy: 0.9353 - val_loss: 3.6996 - val_accuracy: 0.3137\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 0.1257 - accuracy: 0.9552 - val_loss: 3.7473 - val_accuracy: 0.3333\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 2s 357ms/step - loss: 0.1359 - accuracy: 0.9602 - val_loss: 4.1678 - val_accuracy: 0.2941\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.0925 - accuracy: 0.9701 - val_loss: 3.9076 - val_accuracy: 0.2941\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 2s 286ms/step - loss: 0.1099 - accuracy: 0.9602 - val_loss: 3.8863 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "CNN_HBetter = CNN_better.fit(X_trainCNN, y_trainCNN, validation_data=(X_testCNN, y_testCNN), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prediction for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictCNN = CNN.predict_classes(X_testCNN)\n",
    "y_predictCNN_better = CNN_better.predict_classes(X_testCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC(kernel='poly')\n",
    "SVM_better = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training: Adjust Model with Historic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.fit(X_train, y_train)\n",
    "SVM_better.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prediction for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictSVM = SVM.predict(X_test)\n",
    "y_predictSVM_better = SVM_better.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #4 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC = RandomForestClassifier(n_estimators=500)\n",
    "RFC_better = RandomForestClassifier(n_estimators= 400,max_features= 'sqrt',bootstrap= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training: Adjust Model with Historic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_features='sqrt', n_estimators=400)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC.fit(X_train, y_train)\n",
    "RFC_better.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prediction for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictRF = RFC.predict(X_test)\n",
    "y_predictRF_better = RFC_better.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Code\\anaconda3\\envs\\DeepEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Programs\\Code\\anaconda3\\envs\\DeepEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Programs\\Code\\anaconda3\\envs\\DeepEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "CV_LR = cross_val_score(LR, X_train, y_train, cv=3, scoring = \"accuracy\")\n",
    "CV_LR_better = cross_val_score(LR_better, X_train, y_train, cv=3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS_LR = accuracy_score(y_test,y_predictLR)\n",
    "AS_LR_better = accuracy_score(y_test,y_predictLR_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_SVM = cross_val_score(SVM, X_train, y_train, cv=3, scoring = \"accuracy\")\n",
    "CV_SVM_better = cross_val_score(SVM_better, X_train, y_train, cv=3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS_SVM = accuracy_score(y_test,y_predictSVM)\n",
    "AS_SVM_better = accuracy_score(y_test,y_predictSVM_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RF = cross_val_score(RFC, X_train, y_train, cv=3, scoring = \"accuracy\")\n",
    "CV_RF_better = cross_val_score(RFC_better, X_train, y_train, cv=3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS_RF = accuracy_score(y_test,y_predictRF)\n",
    "AS_RF_better = accuracy_score(y_test,y_predictRF_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #5 Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 183ms/step - loss: 0.0528 - accuracy: 0.9851\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0520 - accuracy: 0.9851\n"
     ]
    }
   ],
   "source": [
    "CV_CNN = CNN.evaluate(X_trainCNN, y_trainCNN)\n",
    "CV_CNN_better = CNN_better.evaluate(X_trainCNN, y_trainCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 65ms/step - loss: 5.4562 - accuracy: 0.3529\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.8863 - accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "AS_CNN = CNN.evaluate(X_testCNN, y_testCNN)\n",
    "AS_CNN_better = CNN_better.evaluate(X_testCNN, y_testCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_41424_\">\n",
       "  <caption>Models' Accuracy Scores and Cross Validations</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Accuracy Scores</th>\n",
       "      <th class=\"col_heading level0 col1\" >CV AVG</th>\n",
       "      <th class=\"col_heading level0 col2\" >CV #1</th>\n",
       "      <th class=\"col_heading level0 col3\" >CV #2</th>\n",
       "      <th class=\"col_heading level0 col4\" >CV #3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row0\" class=\"row_heading level0 row0\" >LR</th>\n",
       "      <td id=\"T_41424_row0_col0\" class=\"data row0 col0\" >0.666667</td>\n",
       "      <td id=\"T_41424_row0_col1\" class=\"data row0 col1\" >0.582090</td>\n",
       "      <td id=\"T_41424_row0_col2\" class=\"data row0 col2\" >0.582090</td>\n",
       "      <td id=\"T_41424_row0_col3\" class=\"data row0 col3\" >0.656716</td>\n",
       "      <td id=\"T_41424_row0_col4\" class=\"data row0 col4\" >0.731343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row1\" class=\"row_heading level0 row1\" >SVM</th>\n",
       "      <td id=\"T_41424_row1_col0\" class=\"data row1 col0\" >0.588235</td>\n",
       "      <td id=\"T_41424_row1_col1\" class=\"data row1 col1\" >0.716418</td>\n",
       "      <td id=\"T_41424_row1_col2\" class=\"data row1 col2\" >0.716418</td>\n",
       "      <td id=\"T_41424_row1_col3\" class=\"data row1 col3\" >0.716418</td>\n",
       "      <td id=\"T_41424_row1_col4\" class=\"data row1 col4\" >0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row2\" class=\"row_heading level0 row2\" >RF</th>\n",
       "      <td id=\"T_41424_row2_col0\" class=\"data row2 col0\" >0.745098</td>\n",
       "      <td id=\"T_41424_row2_col1\" class=\"data row2 col1\" >0.671642</td>\n",
       "      <td id=\"T_41424_row2_col2\" class=\"data row2 col2\" >0.671642</td>\n",
       "      <td id=\"T_41424_row2_col3\" class=\"data row2 col3\" >0.701493</td>\n",
       "      <td id=\"T_41424_row2_col4\" class=\"data row2 col4\" >0.686567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row3\" class=\"row_heading level0 row3\" >CNN</th>\n",
       "      <td id=\"T_41424_row3_col0\" class=\"data row3 col0\" >0.352941</td>\n",
       "      <td id=\"T_41424_row3_col1\" class=\"data row3 col1\" >0.985075</td>\n",
       "      <td id=\"T_41424_row3_col2\" class=\"data row3 col2\" >0.985075</td>\n",
       "      <td id=\"T_41424_row3_col3\" class=\"data row3 col3\" >-</td>\n",
       "      <td id=\"T_41424_row3_col4\" class=\"data row3 col4\" >-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row4\" class=\"row_heading level0 row4\" >LR_better</th>\n",
       "      <td id=\"T_41424_row4_col0\" class=\"data row4 col0\" >0.745098</td>\n",
       "      <td id=\"T_41424_row4_col1\" class=\"data row4 col1\" >0.716418</td>\n",
       "      <td id=\"T_41424_row4_col2\" class=\"data row4 col2\" >0.716418</td>\n",
       "      <td id=\"T_41424_row4_col3\" class=\"data row4 col3\" >0.731343</td>\n",
       "      <td id=\"T_41424_row4_col4\" class=\"data row4 col4\" >0.761194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row5\" class=\"row_heading level0 row5\" >SVM_better</th>\n",
       "      <td id=\"T_41424_row5_col0\" class=\"data row5 col0\" >0.686275</td>\n",
       "      <td id=\"T_41424_row5_col1\" class=\"data row5 col1\" >0.746269</td>\n",
       "      <td id=\"T_41424_row5_col2\" class=\"data row5 col2\" >0.746269</td>\n",
       "      <td id=\"T_41424_row5_col3\" class=\"data row5 col3\" >0.746269</td>\n",
       "      <td id=\"T_41424_row5_col4\" class=\"data row5 col4\" >0.731343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row6\" class=\"row_heading level0 row6\" >RF_better</th>\n",
       "      <td id=\"T_41424_row6_col0\" class=\"data row6 col0\" >0.647059</td>\n",
       "      <td id=\"T_41424_row6_col1\" class=\"data row6 col1\" >0.641791</td>\n",
       "      <td id=\"T_41424_row6_col2\" class=\"data row6 col2\" >0.641791</td>\n",
       "      <td id=\"T_41424_row6_col3\" class=\"data row6 col3\" >0.701493</td>\n",
       "      <td id=\"T_41424_row6_col4\" class=\"data row6 col4\" >0.701493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41424_level0_row7\" class=\"row_heading level0 row7\" >CNN_better</th>\n",
       "      <td id=\"T_41424_row7_col0\" class=\"data row7 col0\" >0.333333</td>\n",
       "      <td id=\"T_41424_row7_col1\" class=\"data row7 col1\" >0.985075</td>\n",
       "      <td id=\"T_41424_row7_col2\" class=\"data row7 col2\" >0.985075</td>\n",
       "      <td id=\"T_41424_row7_col3\" class=\"data row7 col3\" >-</td>\n",
       "      <td id=\"T_41424_row7_col4\" class=\"data row7 col4\" >-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x146d1664160>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creación de dataframe con los resultados\n",
    "models = [\"LR\",\"SVM\", \"RF\", \"CNN\",\"LR_better\", \"SVM_better\", \"RF_better\", \"CNN_better\"]\n",
    "AS_values = [AS_LR, AS_SVM, AS_RF,  AS_CNN[1], AS_LR_better, AS_SVM_better, AS_RF_better, AS_CNN_better[1]]\n",
    "CV_values = [CV_LR, CV_SVM, CV_RF, [CV_CNN[1],'-','-'],CV_LR_better,CV_SVM_better, CV_RF_better, [CV_CNN_better[1],'-','-']]\n",
    "\n",
    "data = {\"Accuracy Scores\": AS_values, \"Cross Validations\": CV_values}\n",
    "df = pd.DataFrame(data, index = models)\n",
    "\n",
    "#Separación de arrays de Cross Validations en columnas, referenciando (10)\n",
    "dfCV = pd.DataFrame(df['Cross Validations'].to_list(), columns=['CV #1','CV #2','CV #3'], index = models)\n",
    "#Cálculo de promedio de Cross Validations\n",
    "dfCV.insert(0,'CV AVG', dfCV[['CV #1','CV #2','CV #3']].mean(axis=1, numeric_only=True))\n",
    "\n",
    "#Concatenación de dataframes con las columnas finales, referenciando (11)\n",
    "dfAcc = pd.concat([df['Accuracy Scores'], dfCV], axis=1)\n",
    "dfAcc = dfAcc.style.set_caption(\"Models' Accuracy Scores and Cross Validations\")\n",
    "dfAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 1 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [1 0 4 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 2 1 2 0 0 0 0 0]\n",
      " [2 0 0 0 0 2 0 0 1 2]\n",
      " [0 0 0 2 0 1 3 1 0 0]\n",
      " [0 0 0 0 0 0 1 8 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 2 0 4]]\n",
      "--------------------------------\n",
      "[[3 0 0 0 1 0 0 0 0 0]\n",
      " [0 2 0 0 1 0 0 0 0 0]\n",
      " [0 0 5 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 1 4 0 0 0 0 0]\n",
      " [1 0 0 0 0 5 0 0 0 1]\n",
      " [0 0 0 1 1 1 4 0 0 0]\n",
      " [0 0 0 1 0 0 2 5 0 1]\n",
      " [0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 1 0 0 5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmLR=confusion_matrix(y_test,y_predictLR)\n",
    "print(cmLR)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "cmLR_better=confusion_matrix(y_test,y_predictLR_better)\n",
    "print(cmLR_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar en esta matriz de confusion que al sumar la cantidad de positivos, osea de los valores correctos que mostro (La diagonal) nos dio mayor cantidad el segundo modelo de logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 3 0 0]\n",
      " [0 0 4 0 0 1 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 1 1 0 1 0 4]\n",
      " [3 1 0 1 0 0 0 0 1 1]\n",
      " [3 1 0 5 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 3 2 0 0 0 1 0]]\n",
      "--------------------------------\n",
      "[[0 0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 1 0]\n",
      " [0 0 4 0 0 1 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 3 0]\n",
      " [0 0 0 0 2 1 0 0 0 4]\n",
      " [3 1 0 0 0 0 0 0 1 2]\n",
      " [3 1 0 4 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 0 0]\n",
      " [1 0 0 0 4 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "cmCNN=confusion_matrix(y_test,y_predictCNN)\n",
    "print(cmCNN)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "cmCNN_better=confusion_matrix(y_test,y_predictCNN_better)\n",
    "print(cmCNN_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 1 0 0 0 0 0 0]\n",
      " [0 2 0 0 1 0 0 0 0 0]\n",
      " [1 0 4 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 1 4 0 0 0 0 0]\n",
      " [1 0 0 0 0 3 0 0 1 2]\n",
      " [0 0 0 2 0 1 3 1 0 0]\n",
      " [0 0 0 3 0 0 3 3 0 0]\n",
      " [0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 4]]\n",
      "--------------------------------\n",
      "[[3 0 0 1 0 0 0 0 0 0]\n",
      " [0 2 0 0 1 0 0 0 0 0]\n",
      " [1 0 4 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 1 4 0 0 0 0 0]\n",
      " [1 0 0 0 0 5 0 0 0 1]\n",
      " [0 0 0 1 1 1 4 0 0 0]\n",
      " [0 0 0 3 0 0 3 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 1 0 5]]\n"
     ]
    }
   ],
   "source": [
    "cmSVM=confusion_matrix(y_test,y_predictSVM)\n",
    "print(cmSVM)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "cmSVM_better=confusion_matrix(y_test,y_predictSVM_better)\n",
    "print(cmSVM_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar en esta matriz de confusion que al sumar la cantidad de positivos, osea de los valores correctos que mostro (La diagonal) nos dio mayor cantidad el segundo modelo de Support Vector Machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 5 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 1 4 0 0 0 0 0]\n",
      " [1 0 0 0 0 3 0 0 1 2]\n",
      " [0 0 0 1 0 1 5 0 0 0]\n",
      " [0 0 0 1 0 0 2 5 0 1]\n",
      " [0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 1 0 0 0 1 0 4]]\n",
      "--------------------------------\n",
      "[[3 0 0 1 0 0 0 0 0 0]\n",
      " [0 2 0 0 1 0 0 0 0 0]\n",
      " [0 0 4 0 0 1 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 1 3 0 0 0 0 1]\n",
      " [1 0 0 0 0 3 0 0 1 2]\n",
      " [0 0 0 1 0 2 3 1 0 0]\n",
      " [0 0 0 2 0 0 1 6 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 2 0 0 0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "cmRF=confusion_matrix(y_test,y_predictRF)\n",
    "print(cmRF)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "cmRF_better=confusion_matrix(y_test,y_predictRF_better)\n",
    "print(cmRF_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "498cc7baa86d349c4936134328814e532a1d77a0c0949c12214c3e7da3053f05"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('DeepEnvirom')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
